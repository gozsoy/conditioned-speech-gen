{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper function\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = pd.read_pickle('../data/processed_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_bioguide_id = 'H001055'\n",
    "temp_df = processed_df[processed_df.bioguide_id==temp_bioguide_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.iloc[0].speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add BOS and EOS tokens\n",
    "X_train = list(map(lambda x: ['[BOS]'] + list(x.lower()) + ['[EOS]'], temp_df.speech.values))\n",
    "\n",
    "# merge all texts end to end\n",
    "merged_X_train = np.hstack(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_layer = tf.keras.layers.StringLookup(num_oov_indices=0,output_mode='one_hot')\n",
    "lookup_layer.adapt(merged_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_seq(seq):\n",
    "    input_seq = seq[:,:-1,:] # from char 0 to char [EOS] - 1\n",
    "    target_seq = seq[:,1:,:] # from char 1 to EOS\n",
    "\n",
    "    return input_seq,target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_idx = lookup_layer(merged_X_train)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(X_train_idx)\n",
    "train_ds = train_ds.batch(101,drop_remainder=True).batch(32).map(lambda x: split_seq(x)).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LM(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.lstm1 = tf.keras.layers.LSTM(units=512,return_sequences=True)\n",
    "        self.lstm2 = tf.keras.layers.LSTM(units=256,return_sequences=True)\n",
    "        #self.dense1 = tf.keras.layers.Dense(units=256,activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(units=128,activation='relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(units=len(lookup_layer.get_vocabulary()))\n",
    "\n",
    "    def call(self,x):\n",
    "        \n",
    "        #hiddens = self.lstm2(self.lstm1(x))\n",
    "        #outputs = self.dense3(self.dense2(self.dense1(hiddens)))\n",
    "        hiddens = self.lstm2(x)\n",
    "        outputs = self.dense3(hiddens)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \n",
    "        input_seq,target_seq = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted_seq = self(input_seq)\n",
    "            \n",
    "            loss = self.compiled_loss(target_seq, predicted_seq, regularization_losses=self.losses)\n",
    "\n",
    "        \n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(target_seq, predicted_seq)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_LM()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=4, verbose=1)\n",
    "\n",
    "early_stopper = tf.keras.callbacks.EarlyStopping(monitor='loss',min_delta=0, patience=5, verbose=2)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "model.fit(x=train_ds, epochs=500, verbose=2, callbacks=[early_stopper,lr_scheduler], shuffle=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "initial_text = ''\n",
    "\n",
    "initial_tokens = ['[BOS]'] + list(initial_text)\n",
    "preds_tensor = np.zeros(shape=(1,len(initial_tokens),len(lookup_layer.get_vocabulary())))\n",
    "preds_tensor[0,:,:] = lookup_layer(initial_tokens)\n",
    "preds_tensor = tf.cast(tf.constant(preds_tensor),dtype=tf.float32)\n",
    "decoded_str = initial_text\n",
    "\n",
    "while len(decoded_str) < 100:\n",
    "    last_of_preds = model(preds_tensor)[:,-1:,:]\n",
    "    decoded_ch = lookup_layer.get_vocabulary()[sample(tf.squeeze(tf.math.softmax(last_of_preds)))]\n",
    "    #decoded_ch = lookup_layer.get_vocabulary()[np.argmax(last_of_preds)]\n",
    "    decoded_str += decoded_ch\n",
    "    \n",
    "    preds_tensor = tf.concat([preds_tensor,tf.expand_dims(tf.expand_dims(lookup_layer(decoded_ch),axis=0),axis=0)],axis=1)\n",
    "    \n",
    "print(decoded_str)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad60ff5a86e10c0b348d0515e5915c96ce4bb92811ed18f12804eec31c34dafe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('nlp_lss')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
