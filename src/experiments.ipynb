{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = pd.read_pickle('../data/processed_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_bioguide_id = 'H001055'\n",
    "temp_df = processed_df[processed_df.bioguide_id==temp_bioguide_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list(map(lambda x: ['[BOS]'] + list(x.lower()) + ['[EOS]'], temp_df.speech.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 11:14:39.061089: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "lookup_layer = tf.keras.layers.StringLookup(num_oov_indices=0,output_mode='int')\n",
    "lookup_layer.adapt(tf.ragged.constant(X_train))\n",
    "\n",
    "X_train_idx = lookup_layer(tf.ragged.constant(X_train))\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(X_train_idx)\n",
    "train_ds = train_ds.shuffle(1000).batch(5).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LM(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.lstm1 = tf.keras.layers.LSTM(units=512,return_sequences=True)\n",
    "        self.lstm2 = tf.keras.layers.LSTM(units=256,return_sequences=True)\n",
    "        #self.dense1 = tf.keras.layers.Dense(units=256,activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(units=128,activation='relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(units=len(lookup_layer.get_vocabulary()))\n",
    "\n",
    "    def call(self,x,mask=None):\n",
    "        \n",
    "        #hiddens = self.lstm2(self.lstm1(x))\n",
    "        #outputs = self.dense3(self.dense2(self.dense1(hiddens)))\n",
    "        hiddens = self.lstm2(inputs=x,mask=mask)\n",
    "        outputs = self.dense3(hiddens)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 2.4143\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gokberk/Desktop/eth_courses/nlp_lss/conditioned_speech_gen/src/experiments.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gokberk/Desktop/eth_courses/nlp_lss/conditioned_speech_gen/src/experiments.ipynb#ch0000047?line=30'>31</a>\u001b[0m     loss_value \u001b[39m=\u001b[39m loss_fn(y, y_pred)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gokberk/Desktop/eth_courses/nlp_lss/conditioned_speech_gen/src/experiments.ipynb#ch0000047?line=33'>34</a>\u001b[0m trainable_vars \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtrainable_variables\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gokberk/Desktop/eth_courses/nlp_lss/conditioned_speech_gen/src/experiments.ipynb#ch0000047?line=34'>35</a>\u001b[0m gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss_value, trainable_vars)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gokberk/Desktop/eth_courses/nlp_lss/conditioned_speech_gen/src/experiments.ipynb#ch0000047?line=35'>36</a>\u001b[0m optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(gradients, trainable_vars))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gokberk/Desktop/eth_courses/nlp_lss/conditioned_speech_gen/src/experiments.ipynb#ch0000047?line=37'>38</a>\u001b[0m loss_tracker\u001b[39m.\u001b[39mupdate_state(loss_value)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:1081\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=1076'>1077</a>\u001b[0m \u001b[39mif\u001b[39;00m output_gradients \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=1077'>1078</a>\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=1078'>1079</a>\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m nest\u001b[39m.\u001b[39mflatten(output_gradients)]\n\u001b[0;32m-> <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=1080'>1081</a>\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=1081'>1082</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=1082'>1083</a>\u001b[0m     flat_targets,\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=1083'>1084</a>\u001b[0m     flat_sources,\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=1084'>1085</a>\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=1085'>1086</a>\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=1086'>1087</a>\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=1088'>1089</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=1089'>1090</a>\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=1090'>1091</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py?line=62'>63</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py?line=63'>64</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py?line=64'>65</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py?line=66'>67</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[1;32m     <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py?line=67'>68</a>\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py?line=68'>69</a>\u001b[0m     target,\n\u001b[1;32m     <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py?line=69'>70</a>\u001b[0m     sources,\n\u001b[1;32m     <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py?line=70'>71</a>\u001b[0m     output_gradients,\n\u001b[1;32m     <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py?line=71'>72</a>\u001b[0m     sources_raw,\n\u001b[1;32m     <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py?line=72'>73</a>\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:156\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=153'>154</a>\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=154'>155</a>\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=155'>156</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[1;32m    <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=156'>157</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py?line=157'>158</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py:1741\u001b[0m, in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py?line=1738'>1739</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t_a \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m t_b:\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py?line=1739'>1740</a>\u001b[0m   grad_a \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39mmat_mul(grad, b, transpose_b\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m-> <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py?line=1740'>1741</a>\u001b[0m   grad_b \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39;49mmat_mul(a, grad, transpose_a\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py?line=1741'>1742</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m t_a \u001b[39mand\u001b[39;00m t_b:\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py?line=1742'>1743</a>\u001b[0m   grad_a \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39mmat_mul(grad, b)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:6013\u001b[0m, in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py?line=6010'>6011</a>\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py?line=6011'>6012</a>\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py?line=6012'>6013</a>\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py?line=6013'>6014</a>\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMatMul\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, a, b, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_a\u001b[39;49m\u001b[39m\"\u001b[39;49m, transpose_a, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py?line=6014'>6015</a>\u001b[0m       transpose_b)\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py?line=6015'>6016</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   <a href='file:///Users/gokberk/miniconda3/envs/torch_tf_learning/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py?line=6016'>6017</a>\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = RNN_LM()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "loss_tracker = tf.keras.metrics.Mean(name='categorical_loss')\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for step, batch in enumerate(train_ds):\n",
    "        \n",
    "        # batch processing\n",
    "        max_len = -1\n",
    "        for temp_sample in batch:\n",
    "            if max_len < temp_sample.shape[0]:\n",
    "                max_len = temp_sample.shape[0]\n",
    "        \n",
    "        padded_batch = tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences(list(batch),maxlen=max_len, padding='post',value=-1))\n",
    "        rnn_mask = padded_batch!=-1\n",
    "\n",
    "        one_hot_depth = len(lookup_layer.get_vocabulary())\n",
    "        \n",
    "        padded_batch_one_hot_encoded = tf.one_hot(padded_batch,one_hot_depth)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(padded_batch_one_hot_encoded,rnn_mask)[:,:-1,:] # from char 0 to char [EOS] - 1\n",
    "            y = padded_batch_one_hot_encoded[:,1:,:] # from char 1 to EOS\n",
    "            \n",
    "            loss_value = loss_fn(y, y_pred)\n",
    "\n",
    "        \n",
    "        trainable_vars = model.trainable_variables\n",
    "        gradients = tape.gradient(loss_value, trainable_vars)\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        loss_tracker.update_state(loss_value)\n",
    "        \n",
    "    \n",
    "    epoch_loss = loss_tracker.result()\n",
    "    print(f'epoch: {epoch}, loss: {epoch_loss:.4f}')\n",
    "\n",
    "    loss_tracker.reset_state()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tf.expand_dims(tensor,axis=0)\n",
    "batch_size,seq_len,voc_size = tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LM(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.lstm1 = tf.keras.layers.LSTM(units=512,return_sequences=True)\n",
    "        self.lstm2 = tf.keras.layers.LSTM(units=1024,return_sequences=True)\n",
    "        #self.dense1 = tf.keras.layers.Dense(units=256,activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(units=128,activation='relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(units=len(lookup_layer.get_vocabulary()))\n",
    "\n",
    "    def call(self,x,mask):\n",
    "        \n",
    "        #hiddens = self.lstm2(self.lstm1(x))\n",
    "        #outputs = self.dense3(self.dense2(self.dense1(hiddens)))\n",
    "        hiddens = self.lstm2(inputs=x,mask=mask)\n",
    "        outputs = self.dense3(hiddens)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    '''def train_step(self, x):\n",
    "        # for each batch do the following operation\n",
    "        max_len = -1\n",
    "        for temp_sample in x:\n",
    "            if max_len < temp_sample.shape[0]:\n",
    "                max_len = temp_sample.shape[0]\n",
    "        \n",
    "        padded_batch = tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences(list(i),maxlen=max_len, padding='post',value=-1))\n",
    "        self.rnn_mask = padded_batch!=-1\n",
    "\n",
    "        one_hot_depth = len(lookup_layer.get_vocabulary())\n",
    "        \n",
    "        padded_batch_one_hot_encoded = tf.one_hot(padded_batch,one_hot_depth)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(padded_batch_one_hot_encoded)[:,:-1,:] # from char 0 to char [EOS] - 1\n",
    "            y = x[:,1:,:] # from char 1 to EOS\n",
    "            \n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        \n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_LM()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, verbose=1)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "model.fit(x=padded_batch_one_hot_encoded, epochs=1000, verbose=2, callbacks=[lr_scheduler], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction IMPLEMENT SAMPLING\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "preds_tensor = np.zeros(shape=(1,1,len(lookup_layer.get_vocabulary())))\n",
    "preds_tensor[0,0,:] = lookup_layer('[BOS]')\n",
    "preds_tensor = tf.cast(tf.constant(preds_tensor),dtype=tf.float32)\n",
    "decoded_str = ''\n",
    "\n",
    "while len(decoded_str) < 200:\n",
    "    last_of_preds = model(preds_tensor)[:,-1:,:]\n",
    "    #decoded_ch = lookup_layer.get_vocabulary()[sample(tf.squeeze(tf.math.softmax(last_of_preds)))]\n",
    "    decoded_ch = lookup_layer.get_vocabulary()[np.argmax(last_of_preds)]\n",
    "    decoded_str += decoded_ch\n",
    "    \n",
    "    preds_tensor = tf.concat([preds_tensor,tf.expand_dims(tf.expand_dims(tf.one_hot(lookup_layer(decoded_ch),len(lookup_layer.get_vocabulary())),axis=0),axis=0)],axis=1)\n",
    "\n",
    "print(decoded_str)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad60ff5a86e10c0b348d0515e5915c96ce4bb92811ed18f12804eec31c34dafe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('nlp_lss')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
