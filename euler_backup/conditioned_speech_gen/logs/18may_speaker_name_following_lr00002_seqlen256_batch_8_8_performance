18-05-2022 14:42  - cfg: {'experiment_name': '18may_speaker_name_following_lr00002_seqlen256_batch_8_8', 'data_path': '/cluster/scratch/goezsoy/nlp_lss_datasets', 'log_dir': '../logs', 'checkpoint_dir': '/cluster/scratch/goezsoy/nlp_lss_checkpoints', 'epochs': 10, 'max_seq_len': 256, 'batch_size': 8, 'gradient_accumulations': 8, 'learning_rate': 2e-05, 'print_iter_freq': 500}
18-05-2022 14:42  - Using device: cuda:0
18-05-2022 14:54  - train data loaded.
18-05-2022 14:56  - valid data loaded.

18-05-2022 14:57  - epoch: 1 / 10
18-05-2022 15:11  - iter: 500 / 2898, iter_loss: 1.6792, iter_perplexity: 5.5069
18-05-2022 15:26  - iter: 1000 / 2898, iter_loss: 1.6986, iter_perplexity: 5.9441
18-05-2022 15:40  - iter: 1500 / 2898, iter_loss: 1.6660, iter_perplexity: 5.8708
18-05-2022 15:55  - iter: 2000 / 2898, iter_loss: 1.5189, iter_perplexity: 4.7550
18-05-2022 16:09  - iter: 2500 / 2898, iter_loss: 1.6011, iter_perplexity: 5.1524
18-05-2022 16:23  - epoch: 1 / 10, train_loss: 1.6429, train_perplexity: 6.0055, val_loss: 1.5374, val_perplexity: 5.0594

18-05-2022 16:23  - epoch: 2 / 10
18-05-2022 16:37  - iter: 500 / 2898, iter_loss: 1.5317, iter_perplexity: 4.9871
18-05-2022 16:51  - iter: 1000 / 2898, iter_loss: 1.5012, iter_perplexity: 4.6076
18-05-2022 17:06  - iter: 1500 / 2898, iter_loss: 1.4289, iter_perplexity: 4.2796
18-05-2022 17:20  - iter: 2000 / 2898, iter_loss: 1.4365, iter_perplexity: 4.5160
18-05-2022 17:35  - iter: 2500 / 2898, iter_loss: 1.7980, iter_perplexity: 6.5271
18-05-2022 17:49  - epoch: 2 / 10, train_loss: 1.5275, train_perplexity: 4.9993, val_loss: 1.5002, val_perplexity: 4.8646

18-05-2022 17:49  - epoch: 3 / 10
18-05-2022 18:03  - iter: 500 / 2898, iter_loss: 1.1133, iter_perplexity: 3.2382
18-05-2022 18:17  - iter: 1000 / 2898, iter_loss: 1.3631, iter_perplexity: 4.1774
18-05-2022 18:32  - iter: 1500 / 2898, iter_loss: 1.3001, iter_perplexity: 3.8487
18-05-2022 18:47  - iter: 2000 / 2898, iter_loss: 1.5169, iter_perplexity: 4.7694
18-05-2022 19:01  - iter: 2500 / 2898, iter_loss: 1.3712, iter_perplexity: 4.1939
18-05-2022 19:15  - epoch: 3 / 10, train_loss: 1.4901, train_perplexity: 4.8105, val_loss: 1.4818, val_perplexity: 4.7945

18-05-2022 19:15  - epoch: 4 / 10
18-05-2022 19:29  - iter: 500 / 2898, iter_loss: 1.6524, iter_perplexity: 5.6055
18-05-2022 19:44  - iter: 1000 / 2898, iter_loss: 1.4441, iter_perplexity: 4.8064
18-05-2022 19:58  - iter: 1500 / 2898, iter_loss: 1.6343, iter_perplexity: 5.2067
18-05-2022 20:13  - iter: 2000 / 2898, iter_loss: 1.3655, iter_perplexity: 4.2068
18-05-2022 20:27  - iter: 2500 / 2898, iter_loss: 1.4624, iter_perplexity: 4.6479
18-05-2022 20:41  - epoch: 4 / 10, train_loss: 1.4643, train_perplexity: 4.6742, val_loss: 1.4696, val_perplexity: 4.7115

18-05-2022 20:41  - epoch: 5 / 10
18-05-2022 20:55  - iter: 500 / 2898, iter_loss: 1.2214, iter_perplexity: 3.4472
18-05-2022 21:10  - iter: 1000 / 2898, iter_loss: 1.4651, iter_perplexity: 4.6645
18-05-2022 21:24  - iter: 1500 / 2898, iter_loss: 1.5137, iter_perplexity: 4.6474
18-05-2022 21:39  - iter: 2000 / 2898, iter_loss: 1.3601, iter_perplexity: 3.9960
18-05-2022 21:53  - iter: 2500 / 2898, iter_loss: 1.2993, iter_perplexity: 3.9707
18-05-2022 22:07  - epoch: 5 / 10, train_loss: 1.4441, train_perplexity: 4.5772, val_loss: 1.4619, val_perplexity: 4.6736

18-05-2022 22:07  - epoch: 6 / 10
18-05-2022 22:21  - iter: 500 / 2898, iter_loss: 1.5750, iter_perplexity: 5.4775
18-05-2022 22:36  - iter: 1000 / 2898, iter_loss: 1.3531, iter_perplexity: 4.2332
18-05-2022 22:50  - iter: 1500 / 2898, iter_loss: 1.8252, iter_perplexity: 6.4887
18-05-2022 23:05  - iter: 2000 / 2898, iter_loss: 1.5796, iter_perplexity: 5.0018
18-05-2022 23:19  - iter: 2500 / 2898, iter_loss: 1.2984, iter_perplexity: 3.8769
18-05-2022 23:33  - epoch: 6 / 10, train_loss: 1.4276, train_perplexity: 4.4965, val_loss: 1.4572, val_perplexity: 4.6610

18-05-2022 23:33  - epoch: 7 / 10
18-05-2022 23:48  - iter: 500 / 2898, iter_loss: 1.6202, iter_perplexity: 5.5229
19-05-2022 00:02  - iter: 1000 / 2898, iter_loss: 1.4273, iter_perplexity: 4.4190
19-05-2022 00:17  - iter: 1500 / 2898, iter_loss: 1.4402, iter_perplexity: 4.4150
19-05-2022 00:31  - iter: 2000 / 2898, iter_loss: 1.3497, iter_perplexity: 3.9353
19-05-2022 00:45  - iter: 2500 / 2898, iter_loss: 1.6342, iter_perplexity: 5.3526
19-05-2022 00:59  - epoch: 7 / 10, train_loss: 1.4123, train_perplexity: 4.4206, val_loss: 1.4515, val_perplexity: 4.6180

19-05-2022 00:59  - epoch: 8 / 10
19-05-2022 01:14  - iter: 500 / 2898, iter_loss: 1.6438, iter_perplexity: 5.4966
19-05-2022 01:28  - iter: 1000 / 2898, iter_loss: 1.3229, iter_perplexity: 4.1935
19-05-2022 01:43  - iter: 1500 / 2898, iter_loss: 1.4955, iter_perplexity: 4.5525
19-05-2022 01:57  - iter: 2000 / 2898, iter_loss: 1.3135, iter_perplexity: 4.0713
19-05-2022 02:12  - iter: 2500 / 2898, iter_loss: 1.1407, iter_perplexity: 3.4810
19-05-2022 02:25  - epoch: 8 / 10, train_loss: 1.3988, train_perplexity: 4.3554, val_loss: 1.4494, val_perplexity: 4.6036

19-05-2022 02:25  - epoch: 9 / 10
19-05-2022 02:40  - iter: 500 / 2898, iter_loss: 1.2106, iter_perplexity: 3.8199
