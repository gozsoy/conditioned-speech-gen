09-06-2022 11:40  - cfg: {'model': 'bert_vae', 'experiment_name': '9june_bert_vae_latent200_kl10_maxseqlen256_batch8_8_lr1e4', 'data_path': '/cluster/scratch/goezsoy/nlp_lss_datasets', 'log_dir': '../logs', 'checkpoint_dir': '/cluster/scratch/goezsoy/nlp_lss_checkpoints', 'epochs': 100, 'max_seq_len': 256, 'batch_size': 8, 'gradient_accumulations': 8, 'learning_rate': 0.0001, 'print_iter_freq': 100, 'prefix_len': 10, 'embed_size_per_token': 500, 'speaker_size': 28, 'freeze_gpt': False, 'encoded': 'encoded_parl_part_ids', 'latent_dim': 200, 'kl_weight': 10.0}
09-06-2022 11:40  - Using device: cuda:0
09-06-2022 11:44  - train data loaded.
09-06-2022 11:44  - valid data loaded.

09-06-2022 11:44  - epoch: 1 / 100
09-06-2022 11:48  - iter: 100 / 2898, iter_rec_loss: 6.8066, iter_kl_loss: 0.6333, iter_perplexity: 924.2797
09-06-2022 11:51  - iter: 200 / 2898, iter_rec_loss: 6.6725, iter_kl_loss: 0.1892, iter_perplexity: 799.1105
09-06-2022 11:54  - iter: 300 / 2898, iter_rec_loss: 6.6729, iter_kl_loss: 0.0979, iter_perplexity: 800.3406
09-06-2022 11:58  - iter: 400 / 2898, iter_rec_loss: 6.5615, iter_kl_loss: 0.0733, iter_perplexity: 714.2828
09-06-2022 12:01  - iter: 500 / 2898, iter_rec_loss: 6.5866, iter_kl_loss: 0.0572, iter_perplexity: 736.7712
09-06-2022 12:04  - iter: 600 / 2898, iter_rec_loss: 6.6996, iter_kl_loss: 0.0437, iter_perplexity: 821.0149
09-06-2022 12:08  - iter: 700 / 2898, iter_rec_loss: 6.6405, iter_kl_loss: 0.0436, iter_perplexity: 777.1459
09-06-2022 12:11  - iter: 800 / 2898, iter_rec_loss: 6.6604, iter_kl_loss: 0.0383, iter_perplexity: 785.9539
09-06-2022 12:14  - iter: 900 / 2898, iter_rec_loss: 6.7883, iter_kl_loss: 0.0282, iter_perplexity: 918.0629
09-06-2022 12:18  - iter: 1000 / 2898, iter_rec_loss: 6.7503, iter_kl_loss: 0.0233, iter_perplexity: 864.2258
09-06-2022 12:21  - iter: 1100 / 2898, iter_rec_loss: 6.7111, iter_kl_loss: 0.0267, iter_perplexity: 823.8564
09-06-2022 12:24  - iter: 1200 / 2898, iter_rec_loss: 6.7788, iter_kl_loss: 0.0193, iter_perplexity: 888.9867
09-06-2022 12:28  - iter: 1300 / 2898, iter_rec_loss: 6.8507, iter_kl_loss: 0.0121, iter_perplexity: 979.3338
09-06-2022 12:31  - iter: 1400 / 2898, iter_rec_loss: 6.6614, iter_kl_loss: 0.0128, iter_perplexity: 793.8696
09-06-2022 12:34  - iter: 1500 / 2898, iter_rec_loss: 6.7525, iter_kl_loss: 0.0141, iter_perplexity: 867.4235
09-06-2022 12:38  - iter: 1600 / 2898, iter_rec_loss: 6.7197, iter_kl_loss: 0.0134, iter_perplexity: 851.0041
09-06-2022 12:41  - iter: 1700 / 2898, iter_rec_loss: 6.5645, iter_kl_loss: 0.0128, iter_perplexity: 718.9279
09-06-2022 12:44  - iter: 1800 / 2898, iter_rec_loss: 6.7934, iter_kl_loss: 0.0089, iter_perplexity: 894.0285
09-06-2022 12:48  - iter: 1900 / 2898, iter_rec_loss: 6.7315, iter_kl_loss: 0.0088, iter_perplexity: 849.0940
09-06-2022 12:51  - iter: 2000 / 2898, iter_rec_loss: 6.7015, iter_kl_loss: 0.0096, iter_perplexity: 819.6995
09-06-2022 12:54  - iter: 2100 / 2898, iter_rec_loss: 6.7252, iter_kl_loss: 0.0082, iter_perplexity: 867.8043
09-06-2022 12:58  - iter: 2200 / 2898, iter_rec_loss: 6.6148, iter_kl_loss: 0.0073, iter_perplexity: 750.6855
