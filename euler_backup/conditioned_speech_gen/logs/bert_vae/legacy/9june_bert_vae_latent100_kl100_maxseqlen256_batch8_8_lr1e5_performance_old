09-06-2022 11:33  - cfg: {'model': 'bert_vae', 'experiment_name': '9june_bert_vae_latent100_kl100_maxseqlen256_batch8_8_lr1e5', 'data_path': '/cluster/scratch/goezsoy/nlp_lss_datasets', 'log_dir': '../logs', 'checkpoint_dir': '/cluster/scratch/goezsoy/nlp_lss_checkpoints', 'epochs': 100, 'max_seq_len': 256, 'batch_size': 8, 'gradient_accumulations': 8, 'learning_rate': 1e-05, 'print_iter_freq': 100, 'prefix_len': 10, 'embed_size_per_token': 500, 'speaker_size': 28, 'freeze_gpt': False, 'encoded': 'encoded_parl_part_ids', 'latent_dim': 100, 'kl_weight': 100.0}
09-06-2022 11:33  - Using device: cuda:0
09-06-2022 11:37  - train data loaded.
09-06-2022 11:38  - valid data loaded.

09-06-2022 11:38  - epoch: 1 / 100
09-06-2022 11:41  - iter: 100 / 2898, iter_rec_loss: 6.5373, iter_kl_loss: 16.9663, iter_perplexity: 699.5129
09-06-2022 11:45  - iter: 200 / 2898, iter_rec_loss: 5.1202, iter_kl_loss: 8.3186, iter_perplexity: 168.8870
09-06-2022 11:48  - iter: 300 / 2898, iter_rec_loss: 3.8759, iter_kl_loss: 6.6084, iter_perplexity: 48.6472
09-06-2022 11:51  - iter: 400 / 2898, iter_rec_loss: 2.9376, iter_kl_loss: 5.0125, iter_perplexity: 19.2406
09-06-2022 11:55  - iter: 500 / 2898, iter_rec_loss: 2.1836, iter_kl_loss: 3.8740, iter_perplexity: 9.0591
09-06-2022 11:58  - iter: 600 / 2898, iter_rec_loss: 1.4957, iter_kl_loss: 2.9900, iter_perplexity: 4.4757
09-06-2022 12:01  - iter: 700 / 2898, iter_rec_loss: 1.1680, iter_kl_loss: 2.5996, iter_perplexity: 3.2233
09-06-2022 12:05  - iter: 800 / 2898, iter_rec_loss: 0.8158, iter_kl_loss: 2.2117, iter_perplexity: 2.2664
09-06-2022 12:08  - iter: 900 / 2898, iter_rec_loss: 0.6585, iter_kl_loss: 1.7904, iter_perplexity: 1.9342
09-06-2022 12:11  - iter: 1000 / 2898, iter_rec_loss: 0.5027, iter_kl_loss: 1.4630, iter_perplexity: 1.6571
09-06-2022 12:15  - iter: 1100 / 2898, iter_rec_loss: 0.4074, iter_kl_loss: 1.2600, iter_perplexity: 1.5072
09-06-2022 12:18  - iter: 1200 / 2898, iter_rec_loss: 0.3262, iter_kl_loss: 1.2847, iter_perplexity: 1.3888
09-06-2022 12:21  - iter: 1300 / 2898, iter_rec_loss: 0.2700, iter_kl_loss: 1.0411, iter_perplexity: 1.3116
09-06-2022 12:25  - iter: 1400 / 2898, iter_rec_loss: 0.2215, iter_kl_loss: 0.8513, iter_perplexity: 1.2487
09-06-2022 12:28  - iter: 1500 / 2898, iter_rec_loss: 0.1884, iter_kl_loss: 0.8154, iter_perplexity: 1.2078
09-06-2022 12:31  - iter: 1600 / 2898, iter_rec_loss: 0.1724, iter_kl_loss: 0.6895, iter_perplexity: 1.1885
09-06-2022 12:35  - iter: 1700 / 2898, iter_rec_loss: 0.1544, iter_kl_loss: 0.6589, iter_perplexity: 1.1680
09-06-2022 12:38  - iter: 1800 / 2898, iter_rec_loss: 0.1398, iter_kl_loss: 0.6821, iter_perplexity: 1.1505
09-06-2022 12:41  - iter: 1900 / 2898, iter_rec_loss: 0.1073, iter_kl_loss: 0.5004, iter_perplexity: 1.1134
09-06-2022 12:44  - iter: 2000 / 2898, iter_rec_loss: 0.1185, iter_kl_loss: 0.5504, iter_perplexity: 1.1271
09-06-2022 12:48  - iter: 2100 / 2898, iter_rec_loss: 0.0943, iter_kl_loss: 0.5392, iter_perplexity: 1.0991
09-06-2022 12:51  - iter: 2200 / 2898, iter_rec_loss: 0.0783, iter_kl_loss: 0.4192, iter_perplexity: 1.0817
09-06-2022 12:54  - iter: 2300 / 2898, iter_rec_loss: 0.0834, iter_kl_loss: 0.4687, iter_perplexity: 1.0875
09-06-2022 12:58  - iter: 2400 / 2898, iter_rec_loss: 0.0683, iter_kl_loss: 0.3958, iter_perplexity: 1.0708
