09-06-2022 11:38  - cfg: {'model': 'bert_vae', 'experiment_name': '9june_bert_vae_latent100_kl10_maxseqlen256_batch8_8_lr1e5', 'data_path': '/cluster/scratch/goezsoy/nlp_lss_datasets', 'log_dir': '../logs', 'checkpoint_dir': '/cluster/scratch/goezsoy/nlp_lss_checkpoints', 'epochs': 100, 'max_seq_len': 256, 'batch_size': 8, 'gradient_accumulations': 8, 'learning_rate': 1e-05, 'print_iter_freq': 100, 'prefix_len': 10, 'embed_size_per_token': 500, 'speaker_size': 28, 'freeze_gpt': False, 'encoded': 'encoded_parl_part_ids', 'latent_dim': 100, 'kl_weight': 10.0}
09-06-2022 11:38  - Using device: cuda:0
09-06-2022 11:42  - train data loaded.
09-06-2022 11:43  - valid data loaded.

09-06-2022 11:43  - epoch: 1 / 100
09-06-2022 11:46  - iter: 100 / 2898, iter_rec_loss: 6.2739, iter_kl_loss: 1.4460, iter_perplexity: 533.0967
09-06-2022 11:49  - iter: 200 / 2898, iter_rec_loss: 3.9976, iter_kl_loss: 0.9188, iter_perplexity: 55.4279
09-06-2022 11:53  - iter: 300 / 2898, iter_rec_loss: 2.2664, iter_kl_loss: 0.6834, iter_perplexity: 9.7701
09-06-2022 11:56  - iter: 400 / 2898, iter_rec_loss: 1.3314, iter_kl_loss: 0.5511, iter_perplexity: 3.8094
09-06-2022 11:59  - iter: 500 / 2898, iter_rec_loss: 0.7087, iter_kl_loss: 0.4276, iter_perplexity: 2.0451
09-06-2022 12:02  - iter: 600 / 2898, iter_rec_loss: 0.4672, iter_kl_loss: 0.3531, iter_perplexity: 1.5976
09-06-2022 12:06  - iter: 700 / 2898, iter_rec_loss: 0.2924, iter_kl_loss: 0.2880, iter_perplexity: 1.3422
09-06-2022 12:09  - iter: 800 / 2898, iter_rec_loss: 0.3004, iter_kl_loss: 0.2509, iter_perplexity: 1.3635
09-06-2022 12:12  - iter: 900 / 2898, iter_rec_loss: 0.2110, iter_kl_loss: 0.1792, iter_perplexity: 1.2359
09-06-2022 12:15  - iter: 1000 / 2898, iter_rec_loss: 0.1591, iter_kl_loss: 0.1643, iter_perplexity: 1.1753
09-06-2022 12:19  - iter: 1100 / 2898, iter_rec_loss: 0.1087, iter_kl_loss: 0.1516, iter_perplexity: 1.1155
09-06-2022 12:22  - iter: 1200 / 2898, iter_rec_loss: 0.1157, iter_kl_loss: 0.1298, iter_perplexity: 1.1244
09-06-2022 12:25  - iter: 1300 / 2898, iter_rec_loss: 0.0670, iter_kl_loss: 0.1145, iter_perplexity: 1.0694
09-06-2022 12:28  - iter: 1400 / 2898, iter_rec_loss: 0.0638, iter_kl_loss: 0.0972, iter_perplexity: 1.0660
09-06-2022 12:32  - iter: 1500 / 2898, iter_rec_loss: 0.0585, iter_kl_loss: 0.1023, iter_perplexity: 1.0604
09-06-2022 12:35  - iter: 1600 / 2898, iter_rec_loss: 0.0485, iter_kl_loss: 0.0829, iter_perplexity: 1.0498
09-06-2022 12:38  - iter: 1700 / 2898, iter_rec_loss: 0.0420, iter_kl_loss: 0.0672, iter_perplexity: 1.0429
09-06-2022 12:41  - iter: 1800 / 2898, iter_rec_loss: 0.0304, iter_kl_loss: 0.0661, iter_perplexity: 1.0310
09-06-2022 12:45  - iter: 1900 / 2898, iter_rec_loss: 0.0300, iter_kl_loss: 0.0598, iter_perplexity: 1.0305
09-06-2022 12:48  - iter: 2000 / 2898, iter_rec_loss: 0.0324, iter_kl_loss: 0.0558, iter_perplexity: 1.0330
09-06-2022 12:51  - iter: 2100 / 2898, iter_rec_loss: 0.0259, iter_kl_loss: 0.0509, iter_perplexity: 1.0263
09-06-2022 12:54  - iter: 2200 / 2898, iter_rec_loss: 0.0323, iter_kl_loss: 0.0493, iter_perplexity: 1.0329
09-06-2022 12:58  - iter: 2300 / 2898, iter_rec_loss: 0.0252, iter_kl_loss: 0.0449, iter_perplexity: 1.0255
