15-05-2022 15:20  - cfg: {'experiment_name': 'speaker_name_basic_prompt_lr00001_seqlen256_batch_8_8', 'data_path': '/cluster/scratch/goezsoy/nlp_lss_datasets', 'log_dir': '../logs', 'checkpoint_dir': '../checkpoints', 'epochs': 10, 'max_seq_len': 256, 'batch_size': 8, 'gradient_accumulations': 8, 'learning_rate': 1e-05, 'print_iter_freq': 500}
15-05-2022 15:20  - Using device: cuda:0
15-05-2022 15:28  - train data loaded.
15-05-2022 15:29  - valid data loaded.

15-05-2022 15:30  - epoch: 1 / 10
15-05-2022 15:44  - iter: 500 / 2898, iter_loss: 1.5125, iter_perplexity: 4.8098
15-05-2022 15:59  - iter: 1000 / 2898, iter_loss: 1.6712, iter_perplexity: 5.9582
15-05-2022 16:14  - iter: 1500 / 2898, iter_loss: 1.6092, iter_perplexity: 5.7860
15-05-2022 16:28  - iter: 2000 / 2898, iter_loss: 1.8188, iter_perplexity: 6.4715
15-05-2022 16:43  - iter: 2500 / 2898, iter_loss: 1.5628, iter_perplexity: 4.9746
15-05-2022 16:57  - epoch: 1 / 10, train_loss: 1.7191, train_perplexity: 6.9985, val_loss: 1.5856, val_perplexity: 5.3312

15-05-2022 16:57  - epoch: 2 / 10
15-05-2022 17:12  - iter: 500 / 2898, iter_loss: 1.4123, iter_perplexity: 4.5952
15-05-2022 17:26  - iter: 1000 / 2898, iter_loss: 1.5582, iter_perplexity: 5.1357
15-05-2022 17:41  - iter: 1500 / 2898, iter_loss: 1.5977, iter_perplexity: 5.1070
15-05-2022 17:56  - iter: 2000 / 2898, iter_loss: 1.6292, iter_perplexity: 5.7240
15-05-2022 18:10  - iter: 2500 / 2898, iter_loss: 1.6164, iter_perplexity: 5.3374
15-05-2022 18:24  - epoch: 2 / 10, train_loss: 1.5744, train_perplexity: 5.2620, val_loss: 1.5395, val_perplexity: 5.0817

15-05-2022 18:24  - epoch: 3 / 10
15-05-2022 18:39  - iter: 500 / 2898, iter_loss: 1.5247, iter_perplexity: 4.8177
15-05-2022 18:53  - iter: 1000 / 2898, iter_loss: 1.7367, iter_perplexity: 5.8714
15-05-2022 19:08  - iter: 1500 / 2898, iter_loss: 1.6635, iter_perplexity: 5.8113
