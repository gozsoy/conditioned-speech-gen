15-05-2022 15:02  - cfg: {'experiment_name': 'speaker_name_basic_prompt_lr0001_seqlen256_batch_8_8', 'data_path': '/cluster/scratch/goezsoy/nlp_lss_datasets', 'log_dir': '../logs', 'checkpoint_dir': '../checkpoints', 'epochs': 10, 'max_seq_len': 256, 'batch_size': 8, 'gradient_accumulations': 8, 'learning_rate': 0.0001, 'print_iter_freq': 500}
15-05-2022 15:02  - Using device: cuda:0
15-05-2022 15:10  - train data loaded.
15-05-2022 15:11  - valid data loaded.

15-05-2022 15:11  - epoch: 1 / 10
15-05-2022 15:27  - iter: 500 / 2898, iter_loss: 1.8388, iter_perplexity: 6.5751
15-05-2022 15:43  - iter: 1000 / 2898, iter_loss: 1.5905, iter_perplexity: 5.4020
15-05-2022 15:59  - iter: 1500 / 2898, iter_loss: 1.5351, iter_perplexity: 5.1535
15-05-2022 16:14  - iter: 2000 / 2898, iter_loss: 1.6447, iter_perplexity: 5.6547
15-05-2022 16:30  - iter: 2500 / 2898, iter_loss: 1.8108, iter_perplexity: 6.2954
15-05-2022 16:45  - epoch: 1 / 10, train_loss: 1.5785, train_perplexity: 5.6981, val_loss: 1.4940, val_perplexity: 4.8551

15-05-2022 16:45  - epoch: 2 / 10
15-05-2022 17:01  - iter: 500 / 2898, iter_loss: 1.3866, iter_perplexity: 4.3374
15-05-2022 17:17  - iter: 1000 / 2898, iter_loss: 1.6873, iter_perplexity: 5.5574
15-05-2022 17:33  - iter: 1500 / 2898, iter_loss: 1.4300, iter_perplexity: 4.2407
15-05-2022 17:49  - iter: 2000 / 2898, iter_loss: 1.5398, iter_perplexity: 4.7820
15-05-2022 18:05  - iter: 2500 / 2898, iter_loss: 1.3689, iter_perplexity: 4.3688
15-05-2022 18:20  - epoch: 2 / 10, train_loss: 1.4662, train_perplexity: 4.7056, val_loss: 1.4651, val_perplexity: 4.7097

15-05-2022 18:20  - epoch: 3 / 10
15-05-2022 18:36  - iter: 500 / 2898, iter_loss: 1.3832, iter_perplexity: 4.0748
15-05-2022 18:52  - iter: 1000 / 2898, iter_loss: 1.4707, iter_perplexity: 4.8773
