25-08-2022 00:25  - cfg: {'model': 'k2t', 'experiment_name': '24aug_k2t_gpt2medium_maxseqlen256_batch8_8_lr2e5_partyR', 'data_path': '/cluster/scratch/goezsoy/nlp_lss_datasets', 'log_dir': 'logs', 'checkpoint_dir': '/cluster/scratch/goezsoy/nlp_lss_checkpoints', 'epochs': 3, 'max_seq_len': 256, 'batch_size': 8, 'gradient_accumulations': 8, 'learning_rate': 2e-05, 'print_iter_freq': 500, 'prefix_len': 10, 'embed_size_per_token': 500, 'speaker_size': 28, 'freeze_gpt': False, 'encoded': 'encoded_parl_part_ids', 'latent_dim': 100, 'kl_weight': 1000.0, 'use_annealing': True, 'annealing_period': 200, 'party': 'republican'}
25-08-2022 00:25  - Using device: cuda:0
25-08-2022 00:33  - train data loaded.
25-08-2022 00:34  - valid data loaded.

25-08-2022 00:34  - epoch: 1 / 3
25-08-2022 00:58  - iter: 500 / 2849, iter_loss: 1.9531, iter_perplexity: 7.7716
25-08-2022 01:22  - iter: 1000 / 2849, iter_loss: 2.0087, iter_perplexity: 7.8242
25-08-2022 01:46  - iter: 1500 / 2849, iter_loss: 1.8265, iter_perplexity: 6.3250
25-08-2022 02:10  - iter: 2000 / 2849, iter_loss: 1.8335, iter_perplexity: 6.4958
25-08-2022 02:34  - iter: 2500 / 2849, iter_loss: 1.9183, iter_perplexity: 7.0837
25-08-2022 02:53  - epoch: 1 / 3, train_loss: 1.9120, train_perplexity: 7.4859, val_loss: 1.8101, val_perplexity: 6.4227

25-08-2022 02:54  - epoch: 2 / 3
25-08-2022 03:18  - iter: 500 / 2849, iter_loss: 1.8557, iter_perplexity: 6.5416
25-08-2022 03:42  - iter: 1000 / 2849, iter_loss: 1.9431, iter_perplexity: 7.1749
25-08-2022 04:06  - iter: 1500 / 2849, iter_loss: 1.9190, iter_perplexity: 6.9918
25-08-2022 04:30  - iter: 2000 / 2849, iter_loss: 1.9522, iter_perplexity: 7.3261
25-08-2022 04:54  - iter: 2500 / 2849, iter_loss: 1.8440, iter_perplexity: 6.6455
25-08-2022 05:13  - epoch: 2 / 3, train_loss: 1.8199, train_perplexity: 6.5025, val_loss: 1.7810, val_perplexity: 6.2404

25-08-2022 05:13  - epoch: 3 / 3
25-08-2022 05:37  - iter: 500 / 2849, iter_loss: 1.7181, iter_perplexity: 5.9411
25-08-2022 06:01  - iter: 1000 / 2849, iter_loss: 1.6703, iter_perplexity: 5.5589
25-08-2022 06:25  - iter: 1500 / 2849, iter_loss: 1.8669, iter_perplexity: 7.1363
25-08-2022 06:49  - iter: 2000 / 2849, iter_loss: 1.7767, iter_perplexity: 6.2007
25-08-2022 07:13  - iter: 2500 / 2849, iter_loss: 2.0707, iter_perplexity: 8.3908
25-08-2022 07:33  - epoch: 3 / 3, train_loss: 1.7799, train_perplexity: 6.2436, val_loss: 1.7651, val_perplexity: 6.1542

