25-08-2022 00:05  - cfg: {'model': 'k2t', 'experiment_name': '24aug_k2t_gpt2medium_maxseqlen256_batch8_8_lr2e5_partyD', 'data_path': '/cluster/scratch/goezsoy/nlp_lss_datasets', 'log_dir': 'logs', 'checkpoint_dir': '/cluster/scratch/goezsoy/nlp_lss_checkpoints', 'epochs': 3, 'max_seq_len': 256, 'batch_size': 8, 'gradient_accumulations': 8, 'learning_rate': 2e-05, 'print_iter_freq': 500, 'prefix_len': 10, 'embed_size_per_token': 500, 'speaker_size': 28, 'freeze_gpt': False, 'encoded': 'encoded_parl_part_ids', 'latent_dim': 100, 'kl_weight': 1000.0, 'use_annealing': True, 'annealing_period': 200, 'party': 'democrat'}
25-08-2022 00:05  - Using device: cuda:0
25-08-2022 00:14  - train data loaded.
25-08-2022 00:15  - valid data loaded.

25-08-2022 00:16  - epoch: 1 / 3
25-08-2022 00:40  - iter: 500 / 2980, iter_loss: 2.2149, iter_perplexity: 9.2132
25-08-2022 01:04  - iter: 1000 / 2980, iter_loss: 2.0577, iter_perplexity: 8.5599
25-08-2022 01:29  - iter: 1500 / 2980, iter_loss: 1.9852, iter_perplexity: 7.6369
25-08-2022 01:53  - iter: 2000 / 2980, iter_loss: 2.0163, iter_perplexity: 7.6988
25-08-2022 02:18  - iter: 2500 / 2980, iter_loss: 1.9447, iter_perplexity: 7.2160
25-08-2022 02:44  - epoch: 1 / 3, train_loss: 2.0019, train_perplexity: 7.9470, val_loss: 1.9009, val_perplexity: 7.0051

25-08-2022 02:44  - epoch: 2 / 3
25-08-2022 03:08  - iter: 500 / 2980, iter_loss: 1.9447, iter_perplexity: 7.1963
25-08-2022 03:33  - iter: 1000 / 2980, iter_loss: 1.7270, iter_perplexity: 5.7950
25-08-2022 03:57  - iter: 1500 / 2980, iter_loss: 1.9603, iter_perplexity: 7.3617
25-08-2022 04:22  - iter: 2000 / 2980, iter_loss: 2.0014, iter_perplexity: 7.8269
25-08-2022 04:46  - iter: 2500 / 2980, iter_loss: 2.0414, iter_perplexity: 7.9276
25-08-2022 05:12  - epoch: 2 / 3, train_loss: 1.9166, train_perplexity: 7.1234, val_loss: 1.8741, val_perplexity: 6.8086

25-08-2022 05:12  - epoch: 3 / 3
25-08-2022 05:37  - iter: 500 / 2980, iter_loss: 2.0195, iter_perplexity: 8.1397
25-08-2022 06:01  - iter: 1000 / 2980, iter_loss: 1.8310, iter_perplexity: 6.4014
25-08-2022 06:26  - iter: 1500 / 2980, iter_loss: 1.8462, iter_perplexity: 6.7276
25-08-2022 06:50  - iter: 2000 / 2980, iter_loss: 1.8168, iter_perplexity: 6.4200
25-08-2022 07:14  - iter: 2500 / 2980, iter_loss: 1.9177, iter_perplexity: 6.9627
25-08-2022 07:40  - epoch: 3 / 3, train_loss: 1.8775, train_perplexity: 6.8461, val_loss: 1.8579, val_perplexity: 6.7041

