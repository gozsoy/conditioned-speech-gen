18-08-2022 17:27  - cfg: {'model': 'k2t', 'experiment_name': '18aug_k2t_gpt2medium_maxseqlen256_batch8_8_lr2e5', 'data_path': '/cluster/scratch/goezsoy/nlp_lss_datasets', 'log_dir': 'logs', 'checkpoint_dir': '/cluster/scratch/goezsoy/nlp_lss_checkpoints', 'epochs': 3, 'max_seq_len': 256, 'batch_size': 8, 'gradient_accumulations': 8, 'learning_rate': 2e-05, 'print_iter_freq': 500, 'prefix_len': 10, 'embed_size_per_token': 500, 'speaker_size': 28, 'freeze_gpt': False, 'encoded': 'encoded_parl_part_ids', 'latent_dim': 100, 'kl_weight': 1000.0, 'use_annealing': True, 'annealing_period': 200, 'party': 'all'}
18-08-2022 17:27  - Using device: cuda:0
18-08-2022 17:43  - train data loaded.
18-08-2022 17:44  - valid data loaded.

18-08-2022 17:45  - epoch: 1 / 3
18-08-2022 18:09  - iter: 500 / 5877, iter_loss: 1.8852, iter_perplexity: 6.9384
18-08-2022 18:34  - iter: 1000 / 5877, iter_loss: 1.9825, iter_perplexity: 7.4552
18-08-2022 18:58  - iter: 1500 / 5877, iter_loss: 2.0607, iter_perplexity: 8.5579
18-08-2022 19:22  - iter: 2000 / 5877, iter_loss: 1.9425, iter_perplexity: 7.1450
18-08-2022 19:46  - iter: 2500 / 5877, iter_loss: 2.0347, iter_perplexity: 7.9849
18-08-2022 20:10  - iter: 3000 / 5877, iter_loss: 2.1315, iter_perplexity: 8.9972
18-08-2022 20:35  - iter: 3500 / 5877, iter_loss: 1.9603, iter_perplexity: 7.3814
18-08-2022 20:59  - iter: 4000 / 5877, iter_loss: 1.8453, iter_perplexity: 6.4503
18-08-2022 21:23  - iter: 4500 / 5877, iter_loss: 1.9472, iter_perplexity: 7.3512
18-08-2022 21:48  - iter: 5000 / 5877, iter_loss: 1.7859, iter_perplexity: 6.1483
18-08-2022 22:12  - iter: 5500 / 5877, iter_loss: 1.9065, iter_perplexity: 7.0206
18-08-2022 22:34  - epoch: 1 / 3, train_loss: 1.9289, train_perplexity: 7.3714, val_loss: 1.8350, val_perplexity: 6.5785

18-08-2022 22:35  - epoch: 2 / 3
18-08-2022 22:59  - iter: 500 / 5877, iter_loss: 1.9244, iter_perplexity: 7.2702
18-08-2022 23:23  - iter: 1000 / 5877, iter_loss: 1.9295, iter_perplexity: 7.1864
18-08-2022 23:48  - iter: 1500 / 5877, iter_loss: 1.8876, iter_perplexity: 7.0356
19-08-2022 00:12  - iter: 2000 / 5877, iter_loss: 1.8988, iter_perplexity: 6.7677
19-08-2022 00:36  - iter: 2500 / 5877, iter_loss: 1.8407, iter_perplexity: 6.5802
19-08-2022 01:00  - iter: 3000 / 5877, iter_loss: 1.9294, iter_perplexity: 7.3744
19-08-2022 01:25  - iter: 3500 / 5877, iter_loss: 1.7908, iter_perplexity: 6.2148
19-08-2022 01:49  - iter: 4000 / 5877, iter_loss: 1.9622, iter_perplexity: 7.4129
19-08-2022 02:13  - iter: 4500 / 5877, iter_loss: 1.9071, iter_perplexity: 7.0414
19-08-2022 02:37  - iter: 5000 / 5877, iter_loss: 1.7545, iter_perplexity: 5.8421
19-08-2022 03:02  - iter: 5500 / 5877, iter_loss: 2.0580, iter_perplexity: 7.9757
19-08-2022 03:24  - epoch: 2 / 3, train_loss: 1.8463, train_perplexity: 6.6586, val_loss: 1.8070, val_perplexity: 6.3864

19-08-2022 03:24  - epoch: 3 / 3
19-08-2022 03:48  - iter: 500 / 5877, iter_loss: 1.7983, iter_perplexity: 6.5758
19-08-2022 04:13  - iter: 1000 / 5877, iter_loss: 1.8285, iter_perplexity: 6.3694
19-08-2022 04:37  - iter: 1500 / 5877, iter_loss: 1.9333, iter_perplexity: 7.0182
19-08-2022 05:02  - iter: 2000 / 5877, iter_loss: 1.9862, iter_perplexity: 7.7803
19-08-2022 05:26  - iter: 2500 / 5877, iter_loss: 1.8532, iter_perplexity: 6.7251
19-08-2022 05:51  - iter: 3000 / 5877, iter_loss: 1.6987, iter_perplexity: 5.6184
19-08-2022 06:15  - iter: 3500 / 5877, iter_loss: 1.7280, iter_perplexity: 5.9394
19-08-2022 06:39  - iter: 4000 / 5877, iter_loss: 1.9062, iter_perplexity: 7.0403
19-08-2022 07:04  - iter: 4500 / 5877, iter_loss: 1.6960, iter_perplexity: 6.1285
19-08-2022 07:28  - iter: 5000 / 5877, iter_loss: 1.8050, iter_perplexity: 6.2144
19-08-2022 07:52  - iter: 5500 / 5877, iter_loss: 1.5115, iter_perplexity: 4.7219
19-08-2022 08:15  - epoch: 3 / 3, train_loss: 1.8107, train_perplexity: 6.4161, val_loss: 1.7909, val_perplexity: 6.2928

