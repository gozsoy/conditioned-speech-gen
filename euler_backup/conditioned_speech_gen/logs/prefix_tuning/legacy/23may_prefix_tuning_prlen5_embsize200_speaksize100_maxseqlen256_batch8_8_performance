23-05-2022 16:58  - cfg: {'model': 'prefix_tuning', 'experiment_name': '23may_prefix_tuning_prlen5_embsize200_speaksize100_maxseqlen256_batch8_8', 'data_path': '/cluster/scratch/goezsoy/nlp_lss_datasets', 'log_dir': '../logs', 'checkpoint_dir': '/cluster/scratch/goezsoy/nlp_lss_checkpoints', 'epochs': 100, 'max_seq_len': 256, 'batch_size': 8, 'gradient_accumulations': 8, 'learning_rate': 2e-05, 'print_iter_freq': 500, 'prefix_len': 5, 'embed_size_per_token': 200, 'speaker_size': 100, 'freeze_gpt': False}
23-05-2022 16:58  - Using device: cuda:0
23-05-2022 16:59  - train data loaded.
23-05-2022 16:59  - valid data loaded.

23-05-2022 17:00  - epoch: 1 / 100
23-05-2022 17:10  - epoch: 1 / 100, train_loss: 8.2117, train_perplexity: 50542.8607

23-05-2022 17:10  - epoch: 2 / 100
23-05-2022 17:20  - epoch: 2 / 100, train_loss: 6.7560, train_perplexity: 878.9051

23-05-2022 17:20  - epoch: 3 / 100
23-05-2022 17:30  - epoch: 3 / 100, train_loss: 6.6170, train_perplexity: 761.7274

23-05-2022 17:30  - epoch: 4 / 100
23-05-2022 17:40  - epoch: 4 / 100, train_loss: 6.5424, train_perplexity: 706.7440

23-05-2022 17:40  - epoch: 5 / 100
23-05-2022 17:50  - epoch: 5 / 100, train_loss: 6.4724, train_perplexity: 659.4003

23-05-2022 17:50  - epoch: 6 / 100
23-05-2022 18:01  - epoch: 6 / 100, train_loss: 6.3985, train_perplexity: 613.2580

23-05-2022 18:01  - epoch: 7 / 100
23-05-2022 18:11  - epoch: 7 / 100, train_loss: 6.3064, train_perplexity: 559.5201

23-05-2022 18:11  - epoch: 8 / 100
23-05-2022 18:21  - epoch: 8 / 100, train_loss: 6.1970, train_perplexity: 502.2165

23-05-2022 18:21  - epoch: 9 / 100
23-05-2022 18:31  - epoch: 9 / 100, train_loss: 6.0714, train_perplexity: 443.4000

23-05-2022 18:31  - epoch: 10 / 100
23-05-2022 18:41  - epoch: 10 / 100, train_loss: 5.9275, train_perplexity: 384.9197

23-05-2022 18:41  - epoch: 11 / 100
23-05-2022 18:51  - epoch: 11 / 100, train_loss: 5.7687, train_perplexity: 328.6214

23-05-2022 18:51  - epoch: 12 / 100
23-05-2022 19:02  - epoch: 12 / 100, train_loss: 5.5960, train_perplexity: 277.3847

23-05-2022 19:02  - epoch: 13 / 100
23-05-2022 19:12  - epoch: 13 / 100, train_loss: 5.4232, train_perplexity: 233.5795

23-05-2022 19:12  - epoch: 14 / 100
23-05-2022 19:22  - epoch: 14 / 100, train_loss: 5.2606, train_perplexity: 199.1088

23-05-2022 19:22  - epoch: 15 / 100
23-05-2022 19:32  - epoch: 15 / 100, train_loss: 5.1144, train_perplexity: 172.1422

23-05-2022 19:32  - epoch: 16 / 100
23-05-2022 19:42  - epoch: 16 / 100, train_loss: 4.9802, train_perplexity: 151.2079

23-05-2022 19:42  - epoch: 17 / 100
23-05-2022 19:53  - epoch: 17 / 100, train_loss: 4.8634, train_perplexity: 134.6965

23-05-2022 19:53  - epoch: 18 / 100
23-05-2022 20:03  - epoch: 18 / 100, train_loss: 4.7600, train_perplexity: 121.7275

23-05-2022 20:03  - epoch: 19 / 100
23-05-2022 20:13  - epoch: 19 / 100, train_loss: 4.6656, train_perplexity: 110.9835

23-05-2022 20:13  - epoch: 20 / 100
23-05-2022 20:23  - epoch: 20 / 100, train_loss: 4.5824, train_perplexity: 102.2009

23-05-2022 20:23  - epoch: 21 / 100
23-05-2022 20:33  - epoch: 21 / 100, train_loss: 4.5021, train_perplexity: 94.3048

23-05-2022 20:33  - epoch: 22 / 100
23-05-2022 20:43  - epoch: 22 / 100, train_loss: 4.4286, train_perplexity: 87.8082

23-05-2022 20:43  - epoch: 23 / 100
23-05-2022 20:54  - epoch: 23 / 100, train_loss: 4.3602, train_perplexity: 82.2006

23-05-2022 20:54  - epoch: 24 / 100
