## TODO

1- no bert finetuning to see if loss is large (sanity check)
political word embeddings

- wise truncation


## DONE
0- try with only single example -> overfit -> generate text out of it (pipeline on notebooks?)
4- reduce GPU memory usage: freezing some parts (bottom 6) -> no, reducing max_token_size -> good
3- perplexity computation wrong? (not, actually!)
5- higher quality logging and tensorboard
1- generate some text per epoch -> how to use tensorboard in Euler? (add_text)
1- use larger subset of speakers -> use validation data
3- 512_4 generation results?? -> done via multiple job submission
0- unconditioned model text generation ("The following is a speech by Travis Childers." or "Travis Childers")
1- observe generation quality when perplexity around 20, not 1 (1st epoch's initial batches)
4- topk and topp sampling (3 different generation scenarios at 1 go?)
3- train with different prompts: "Travis Childers says the following:"(NO) vs "Travis Childers"(YES) vs "The following is a speech by Travis Childers."(YES)
2- finetune gpt as well (allows cross contamination) or fine-tune for 1 epoch then freeze?
tsne plot per epoch to see changes in clusters
- prefix tuning -> training with only most frequent 50 speakers
- prefix tuning -> state specific training
0- kl weight annealing


## MAYBE
5- reduce GPU memory usage: use distilgpt2
6- huggingface Accelerate you have full control over the training loop and can essentially write the loop in pure PyTorch with some minor modifications
9- Using gpt-2 novelty bleu or rouge scores
- speaker name prompted fine tuning. during generation prompt-> "Travis Childers talking about new legislation year in Nevada"
- add callbacks: lr scheduler, early stopper

# experiment names:
speaker_name_basic_prompt -> row['first_name'] + ' ' + row['last_name'] + row['speech']
speaker_name_following -> 'The following is a speech by '+ row['first_name'] + ' ' + row['last_name'] + '.' + row['speech']