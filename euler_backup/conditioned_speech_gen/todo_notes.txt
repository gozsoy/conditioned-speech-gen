## TODO

1- generate some text per epoch -> how to use tensorboard in Euler? (add_text)

2- use larger subset of speakers -> use validation data
3- wise truncation -> perplexity computation wrong? (not, actually!)

4- reduce GPU memory usage: reducing max_token_size (promising), use distilgpt2

5- put dots for each iteration within epoch

6- huggingface Accelerate you have full control over the training loop and can essentially write the loop in pure PyTorch with some minor modifications

## DONE
0- try with only single example -> overfit -> generate text out of it (pipeline on notebooks?)
4- reduce GPU memory usage: freezing some parts (bottom 6) -> no