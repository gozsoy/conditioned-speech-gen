{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# read whole corpus\n",
    "processed_df = pd.read_csv('/cluster/scratch/goezsoy/nlp_lss_datasets/processed_df.csv')\n",
    "\n",
    "# work on subset of speakers\n",
    "temp_bioguides = ['H001055','C001074']\n",
    "temp_df = processed_df[processed_df.bioguide_id.isin(temp_bioguides)]\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/cluster/scratch/goezsoy/miniconda3/envs/cond_text_gen_project/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "temp_df.loc[87084].speech"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\". Mr. Speaker, I rise today to recognize the outstanding career of Dr. John W. Becher, or ``Chief'' as he was called by scores of medical residents, an osteopathic physician who has dedicated his life to his patients, his students, and to the improvement of the medical profession.  Dr. Becher's commitment to the field of emergency medicine spans more than 30 years. As professor and chairman of the Department of Emergency Medicine at the Philadelphia College of Osteopathic Medicine, he has helped countless students and residents, myself included, develop their skills and become an essential part of our health care workforce.  As a young resident at Albert Einstein Medical Center, I was fortunate to have Dr. Becher's insight and guidance as my residency director. His dedication to emergency medicine was evident then, and his understanding of the osteopathic profession was invaluable to my training and to my career.  His involvement in the field of osteopathic medicine is unparalleled. In addition to his work at PCOM, he currently serves as the secretary treasurer of the National Board of Osteopathic Medical Examiners and is a member of the board of trustees for the American Osteopathic Association.  He was a member of the editorial board of the Journal of the American Osteopathic Association for nearly 20 years, and he is the past president of the American College of Osteopathic Emergency Physicians--and these are only some of his accomplishments. His never-ending contributions and service to his profession and his patients have rightly been recognized, most recently by the awarding of the O.J. Snyder Memorial Medal.  Dr. Becher's lifelong commitment to patient care and to the excellence of future physicians serves as a powerful legacy to the field of emergency medicine. I consider myself fortunate to have learned under his leadership, and it is an honor to recognize his achievements.  Chief, my sincere congratulations on your well-deserved retirement.\""
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# loading model for generation\n",
    "from model import Net\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "checkpoint = torch.load('../checkpoints/speaker_name_as_prompt_deneme_best.pt',map_location=device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Net(\n",
       "  (gpt_neo): GPTNeoForCausalLM(\n",
       "    (transformer): GPTNeoModel(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(2048, 768)\n",
       "      (drop): Dropout(p=0, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "inputs = tokenizer('Michael Jordan says', return_tensors=\"pt\")\n",
    "\n",
    "gen_tokens = model.gpt_neo.generate(\n",
    "    inputs.input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.4,\n",
    "    max_length=100,\n",
    ")\n",
    "tokenizer.batch_decode(gen_tokens)[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Michael Jordan says: â€œI want to thank the gentleman from across the aisle for his kind remarks.  Mr. Speaker, I rise today to recognize the outstanding career of Dr. John W. Becher, or ``Chief'' as he was called by scores of medical residents, an osteopathic physician who has dedicated his life to his patients, his students, and to the improvement of the medical profession.  Dr. Becher's commitment to the field of emergency medicine spans more than 30 years\""
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('cond_text_gen_project': conda)"
  },
  "interpreter": {
   "hash": "7918bf550cc222160dde757c7164fa6a3057e939eb504426b669afaa98f6c6cb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}