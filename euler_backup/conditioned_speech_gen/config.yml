# experiment
model: prefix_tuning # speaker_prompt, prefix_tuning, vae_bert
experiment_name: 25may_prefix_tuning_pp_prlen10_embsize500_speaksize28_maxseqlen256_batch8_8 # prefix_tuning_spt or prefix_tuning_pp

# directory info
data_path: /cluster/scratch/goezsoy/nlp_lss_datasets
log_dir: ../logs
checkpoint_dir: /cluster/scratch/goezsoy/nlp_lss_checkpoints

# training
epochs: 100
max_seq_len: 256
batch_size: 8
gradient_accumulations: 8  # batch_size * gr_acc = effective batch size
learning_rate: 0.0001 # 1e-5?
print_iter_freq: 500
#early_stopping: True  # fix
#lr_scheduling: True  # fix

# generation
#do_sample: True
#temperature: 0.9
#max_length: 200

# prefix tuning
prefix_len: 10
embed_size_per_token: 500
speaker_size: 28 #800 or 106 or 28
freeze_gpt: False
encoded: 'encoded_parl_part_ids' # 'encoded_bioguide_ids' or 'encoded_st_p_ids' or 'encoded_parl_part_ids'