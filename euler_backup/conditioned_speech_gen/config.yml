# experiment
model: k2t # speaker_prompt, prefix_tuning, bert_vae, k2t
experiment_name: 19july_k2t_gpt2medium_maxseqlen256_batch8_8_lr2e5 # prefix_tuning_spt or prefix_tuning_pp

# directory info
data_path: /cluster/scratch/goezsoy/nlp_lss_datasets
log_dir: ../logs
checkpoint_dir: /cluster/scratch/goezsoy/nlp_lss_checkpoints

# training
epochs: 10
max_seq_len: 256
batch_size: 8
gradient_accumulations: 8  # batch_size * gr_acc = effective batch size
learning_rate: 0.00002 # 1e-5?
print_iter_freq: 500
#early_stopping: True  # fix
#lr_scheduling: True  # fix

# generation
#do_sample: True
#temperature: 0.9
#max_length: 200

# prefix tuning
prefix_len: 10
embed_size_per_token: 500
speaker_size: 28 #800 or 106 or 28
freeze_gpt: False
encoded: 'encoded_parl_part_ids' # 'encoded_bioguide_ids' or 'encoded_st_p_ids' or 'encoded_parl_part_ids'

# bert_vae
latent_dim: 100
kl_weight: 1000.0
use_annealing: True
annealing_period: 200

# k2t
party: all  # all, democrat, republican